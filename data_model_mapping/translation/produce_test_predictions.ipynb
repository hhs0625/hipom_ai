{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: end to end inference and evaluation\n",
    "\n",
    "given a csv, make predictions and evaluate predictions, then return results in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = f\"../make_data/test.xlsx\"\n",
    "# Ensure to include 'ships_idx' in the fields list\n",
    "fields = ['ships_idx', 'tag_name', 'tag_description', 'thing', 'property']\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(data_path, usecols=fields)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_excel(data_path, usecols=fields, encoding='ISO-8859-1')\n",
    "\n",
    "\n",
    "# df = df.dropna().reset_index(drop=True)\n",
    "selected_columns = ['thing', 'property', 'tag_description']\n",
    "df[selected_columns] = df[selected_columns].astype(\"string\")\n",
    "df['ships_idx'] = df['ships_idx'].astype(\"Int64\")\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59091"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternate slower tag_description processing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_desc= df['tag_description'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_strings(strings, diff_count):\n",
    "    \n",
    "    family_of_f = []\n",
    "    ignore_list = set()\n",
    "    first_time = True\n",
    "    counter = 0 \n",
    "    for i in tqdm.tqdm(range(len(strings))):\n",
    "        # print(\"i is \", i)\n",
    "        if (strings[i] in ignore_list):\n",
    "            continue\n",
    "        family = []\n",
    "        for j in range(i + 1,len(strings)):\n",
    "            # print(\"j is \", j)\n",
    "            if (strings[j] in ignore_list):\n",
    "                continue\n",
    "            mismatch_count = 0\n",
    "\n",
    "            # find number of corresponding differences\n",
    "            if len(strings[i]) == len(strings[j]):\n",
    "                for k in range(len(strings[i])):\n",
    "                    if strings[i][k] != strings[j][k]:\n",
    "                        mismatch_count += 1\n",
    "                        if mismatch_count > diff_count:\n",
    "                            break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if (mismatch_count > 0 and mismatch_count <= diff_count and first_time):\n",
    "                family.append((strings[i], i))\n",
    "                family.append((strings[j], j))\n",
    "                ignore_list.add(strings[i])\n",
    "                ignore_list.add(strings[j])\n",
    "                first_time = False\n",
    "            elif (mismatch_count > 0 and mismatch_count <= diff_count and (not first_time)):\n",
    "                family.append((strings[j], j))\n",
    "                ignore_list.add(strings[j])\n",
    "            else:\n",
    "               pass \n",
    "        if (len(family)) > 0:\n",
    "            family_of_f.append(family)\n",
    "        first_time = True\n",
    "        \n",
    "    return family_of_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/59091 [00:00<03:19, 296.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59091/59091 [00:56<00:00, 1052.72it/s]\n"
     ]
    }
   ],
   "source": [
    "family_list = find_matching_strings(tag_desc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('family_list.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(family_list, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode \n",
    "import pickle\n",
    "with open('family_list.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    family_list = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(family_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff_char(strings):\n",
    "    reference = strings[0][0]\n",
    "    flank_pos = set()\n",
    "    for sample in strings:\n",
    "        for i in range(len(reference)):\n",
    "            if reference[i] != sample[0][i]:\n",
    "                flank_pos.add(i)\n",
    "    return flank_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to insert <attn>\n",
    "def mutate_list(tag_desc, family):\n",
    "    indices = find_diff_char(family)\n",
    "    for num in range(len(indices)):\n",
    "        # re-compute differences in family after a single pass\n",
    "        updated_indices = find_diff_char(family)\n",
    "        # since we already know how many differences there are\n",
    "        # and the positions are by index positions\n",
    "        # we just take the values and sort and take the num-th value\n",
    "        updated_indices = sorted(list(updated_indices))\n",
    "        for index, tuple in enumerate(family):\n",
    "            word = tuple[0]\n",
    "            id = updated_indices[num]\n",
    "            part1 = word[:id]\n",
    "            part2 = word[id]\n",
    "            part3 = word[id + 1:]\n",
    "            tag_desc[tuple[1]] = part1 + \"<attn>\" + part2 + \"<attn>\" + part3\n",
    "            word = tag_desc[tuple[1]]\n",
    "            # update family item too\n",
    "            family[index] = (word, tuple[1])\n",
    "\n",
    "\n",
    "    # for tuple in family:\n",
    "    #     print(tag_desc[tuple[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it works\n",
    "# mutate_list(tag_desc, family_list[0])\n",
    "# family_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in family_list:\n",
    "    mutate_list(tag_desc, family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<attn>E<attn><attn>C<attn><attn>S<attn> ABNORMAL', 847),\n",
       " ('<attn>P<attn><attn>M<attn><attn>S<attn> ABNORMAL', 4181),\n",
       " ('<attn>E<attn><attn>/<attn><attn>G<attn> ABNORMAL', 57840)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_list[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M/E COMMON ALM FROM EICU-<attn>A<attn> ', 456),\n",
       " ('M/E COMMON ALM FROM EICU-<attn>B<attn> ', 457)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_list[116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result back to df\n",
    "new_df = pd.DataFrame({'tag_description_with_tokens': tag_desc})\n",
    "df = pd.concat([df, new_df], axis=1)\n",
    "df['tag_description_with_tokens'] = df['tag_description_with_tokens'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def process_df(df):\n",
    "    output_list = [{\n",
    "        'translation': {\n",
    "            'ships_idx': row['ships_idx'],\n",
    "            'tag_description': row['tag_description_with_tokens'],\n",
    "            'thing_property': f\"<THING_START>{row['thing']}<THING_END><PROPERTY_START>{row['property']}<PROPERTY_END>\",\n",
    "            'answer_thing': f\"{row['thing']}\",\n",
    "            'answer_property':f\"{row['property']}\",\n",
    "        }\n",
    "    } for _, row in df.iterrows()]\n",
    "\n",
    "    return output_list\n",
    "\n",
    "test_dataset = Dataset.from_list(process_df(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# model_checkpoint = \"train_tp_checkpoint_80/checkpoint-4640\"\n",
    "# model_checkpoint = \"train_tp_checkpoint_40/checkpoint-2760\"\n",
    "# model_checkpoint = \"checkpoint_attention_token_20/checkpoint-1380\"\n",
    "model_checkpoint = \"checkpoint_attention_token_40/checkpoint-2760\"\n",
    "\n",
    "# model_checkpoint = \"checkpoint_reference_20/checkpoint-1380\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", return_tensors=\"pt\")\n",
    "# Define additional special tokens\n",
    "additional_special_tokens = [\"<THING_START>\", \"<THING_END>\", \"<PROPERTY_START>\", \"<PROPERTY_END>\", \"<attn>\"]\n",
    "# Add the additional special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n",
    "# tokenizer.add_special_tokens({'sep_token': \"<SEP>\"})\n",
    "\n",
    "\n",
    "pipe = pipeline(\"translation_XX_to_YY\", model=model_checkpoint, tokenizer=tokenizer, return_tensors=True, max_length=64, device=0)\n",
    "\n",
    "# check what token-ids the special tokens are\n",
    "# tokenizer.encode(\"<THING_START><THING_END><PROPERTY_START><PROPERTY_END>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seq(tokens, start_value, end_value):\n",
    "    if start_value not in tokens or end_value not in tokens:\n",
    "        return None  # Or handle this case according to your requirements\n",
    "    start_id = tokens.index(start_value)\n",
    "    end_id = tokens.index(end_value)\n",
    "\n",
    "    return tokens[start_id+1:end_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem, what if end tokens are not in?\n",
    "def process_tensor_output(output):\n",
    "    tokens = output[0]['translation_token_ids'].tolist()\n",
    "    thing_seq = extract_seq(tokens, 32100, 32101) # 32100 = <THING_START>, 32101 = <THING_END>\n",
    "    property_seq = extract_seq(tokens, 32102, 32103) # 32102 = <PROPERTY_START>, 32103 = <PROPERTY_END>\n",
    "    p_thing = None\n",
    "    p_property = None\n",
    "    if (thing_seq is not None):\n",
    "        p_thing =  tokenizer.decode(thing_seq, skip_special_tokens=True)\n",
    "    if (property_seq is not None):\n",
    "        p_property =  tokenizer.decode(property_seq, skip_special_tokens=True)\n",
    "    return p_thing, p_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making inference on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59091it [02:32, 388.59it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_thing_list = []\n",
    "p_property_list = []\n",
    "print(\"making inference on test set\")\n",
    "for out in tqdm(pipe(KeyDataset(test_dataset[\"translation\"], \"tag_description\"), batch_size=256)):\n",
    "    p_thing, p_property = process_tensor_output(out)\n",
    "    p_thing_list.append(p_thing)\n",
    "    p_property_list.append(p_property)\n",
    "print(\"inference done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing prediction accuracy 0.15027669188201248\n",
      "property prediction accuracy 0.14665515899206308\n"
     ]
    }
   ],
   "source": [
    "answer_thing = [ item['answer_thing'] for item in test_dataset[\"translation\"]]\n",
    "answer_property = [ item['answer_property'] for item in test_dataset[\"translation\"]]\n",
    "def correctness_test(input, reference):\n",
    "    assert(len(input) == len(reference))\n",
    "    correctness_list = []\n",
    "    for i in range(0,len(input)):\n",
    "        if (input[i] == reference[i]):\n",
    "            correctness_list.append(True)\n",
    "        else:\n",
    "            correctness_list.append(False)\n",
    "    return correctness_list\n",
    "\n",
    "# compare with answer to evaluate correctness\n",
    "thing_correctness = correctness_test(p_thing_list, answer_thing)\n",
    "print(\"thing prediction accuracy\", sum(thing_correctness)/len(thing_correctness))\n",
    "property_correctness = correctness_test(p_property_list, answer_property)\n",
    "print(\"property prediction accuracy\", sum(property_correctness)/len(property_correctness))\n",
    "\n",
    "dict = {'p_thing': p_thing_list, \n",
    "        'p_property': p_property_list,\n",
    "        'p_thing_correct': thing_correctness,\n",
    "        'p_property_correct': property_correctness}\n",
    "df_pred = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset again, this time with all fields, except last 5 fields\n",
    "data_path = f\"../make_data/test.xlsx\"\n",
    "# Load the dataset\n",
    "\n",
    "fields = ['thing',  'property', 'ships_idx', 'tag_name', 'equip_type_code', 'tag_description',\n",
    "        'tx_period', 'tx_type', 'on_change_yn', 'scaling_const', 'signal_type', 'min',\n",
    "        'max', 'unit', 'data_type', 'description', 'updated_time', 'status_code',\n",
    "        'is_timeout']\n",
    "\n",
    "df_orig = pd.read_excel(data_path, usecols=fields)\n",
    "# try:\n",
    "#     df_orig = pd.read_csv(data_path, usecols=fields, skipinitialspace=True)\n",
    "# except UnicodeDecodeError:\n",
    "#     df_orig = pd.read_csv(data_path, usecols=fields, skipinitialspace=True, encoding='ISO-8859-1')\n",
    "\n",
    "columns_to_check = ['ships_idx', 'tag_name', 'tag_description', 'thing', 'property']\n",
    "# df_orig = df_orig.dropna(subset=columns_to_check).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# combine prediction dataframe\n",
    "df_final = pd.concat([df_orig, df_pred], axis=1)\n",
    "# df_final.to_csv('test_with_predictions.csv')\n",
    "# df_final.to_excel('test_with_predictions.xlsx')\n",
    "df_final.to_parquet('test_with_predictions.parquet', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('test_with_predictions.xlsx')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# restricted fields\n",
    "# load dataset again, this time with all fields, except last 5 fields\n",
    "data_path = f\"test.csv\"\n",
    "# Load the dataset\n",
    "\n",
    "fields = ['thing',  'property', 'ships_idx', 'tag_name', 'tag_description']\n",
    "\n",
    "try:\n",
    "    df_orig = pd.read_csv(data_path, usecols=fields, skipinitialspace=True)\n",
    "except UnicodeDecodeError:\n",
    "    df_orig = pd.read_csv(data_path, usecols=fields, skipinitialspace=True, encoding='ISO-8859-1')\n",
    "df_orig = df_orig.dropna()\n",
    "\n",
    "# # combine prediction dataframe\n",
    "df_final = pd.concat([df_orig, df_pred], axis=1)\n",
    "# # df_final.to_csv('test_with_predictions.csv')\n",
    "df_final.to_excel('test_with_predictions_limit_fields.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
