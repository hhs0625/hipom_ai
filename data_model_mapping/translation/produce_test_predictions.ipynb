{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: end to end inference and evaluation\n",
    "\n",
    "given a csv, make predictions and evaluate predictions, then return results in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_org:\n",
      "['thing', 'property', 'ships_idx', 'tag_name', 'equip_type_code', 'tag_description', 'tx_period', 'tx_type', 'on_change_yn', 'scaling_const', 'signal_type', 'min', 'max', 'data_type', 'description', 'updated_time', 'status_code', 'is_timeout', 'p_thing', 'p_property', 'p_thing_correct', 'p_property_correct', 'MDM', 'pattern']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594509/4046731478.py:7: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"../make_test_csv/test.csv\"  # Adjust the CSV file path as necessary\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Drop rows where 'tag_description' is NaN and reset the index\n",
    "df = df.dropna(subset=['tag_description']).reset_index(drop=True)\n",
    "\n",
    "# Preserve df_org\n",
    "df_org = df.copy()\n",
    "\n",
    "# Print the column names of df_org\n",
    "print(\"Columns in df_org:\")\n",
    "print(df_org.columns.tolist())\n",
    "\n",
    "selected_columns = ['thing', 'property', 'tag_description']\n",
    "df[selected_columns] = df[selected_columns].astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test_dataset contains 59058 items.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def process_df(df):\n",
    "    output_list = [{\n",
    "        'translation': {\n",
    "            'ships_idx': row['ships_idx'],\n",
    "            'tag_description': row['tag_description'],\n",
    "            'thing_property': f\"<THING_START>{row['thing']}<THING_END><PROPERTY_START>{row['property']}<PROPERTY_END>\",\n",
    "            'answer_thing': f\"{row['thing']}\",\n",
    "            'answer_property':f\"{row['property']}\",\n",
    "        }\n",
    "    } for _, row in df.iterrows()]\n",
    "\n",
    "    return output_list\n",
    "\n",
    "# Process the DataFrame\n",
    "processed_data = process_df(df)\n",
    "\n",
    "# Create a Dataset object\n",
    "test_dataset = Dataset.from_list(processed_data)\n",
    "\n",
    "# Print the number of items in the dataset\n",
    "print(f\"The test_dataset contains {len(test_dataset)} items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_checkpoint = \"train_tp_checkpoint_1/checkpoint-2240\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", return_tensors=\"pt\")\n",
    "# Define additional special tokens\n",
    "additional_special_tokens = [\"<THING_START>\", \"<THING_END>\", \"<PROPERTY_START>\", \"<PROPERTY_END>\"]\n",
    "# Add the additional special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n",
    "# tokenizer.add_special_tokens({'sep_token': \"<SEP>\"})\n",
    "\n",
    "\n",
    "pipe = pipeline(\"translation_XX_to_YY\", model=model_checkpoint, tokenizer=tokenizer, return_tensors=True, max_length=64, device=0)\n",
    "\n",
    "# check what token-ids the special tokens are\n",
    "# tokenizer.encode(\"<THING_START><THING_END><PROPERTY_START><PROPERTY_END>\")\n",
    "\n",
    "def extract_seq(tokens, start_value, end_value):\n",
    "    if start_value not in tokens or end_value not in tokens:\n",
    "        return None  # Or handle this case according to your requirements\n",
    "    start_id = tokens.index(start_value)\n",
    "    end_id = tokens.index(end_value)\n",
    "\n",
    "    return tokens[start_id+1:end_id]\n",
    "\n",
    "# problem, what if end tokens are not in?\n",
    "def process_tensor_output(output):\n",
    "    tokens = output[0]['translation_token_ids'].tolist()\n",
    "    thing_seq = extract_seq(tokens, 32100, 32101) # 32100 = <THING_START>, 32101 = <THING_END>\n",
    "    property_seq = extract_seq(tokens, 32102, 32103) # 32102 = <PROPERTY_START>, 32103 = <PROPERTY_END>\n",
    "    p_thing = None\n",
    "    p_property = None\n",
    "    if (thing_seq is not None):\n",
    "        p_thing =  tokenizer.decode(thing_seq)\n",
    "    if (property_seq is not None):\n",
    "        p_property =  tokenizer.decode(property_seq)\n",
    "    return p_thing, p_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making inference on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59058it [01:44, 566.57it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_thing_list = []\n",
    "p_property_list = []\n",
    "print(\"making inference on test set\")\n",
    "for out in tqdm(pipe(KeyDataset(test_dataset[\"translation\"], \"tag_description\"), batch_size=256)):\n",
    "    p_thing, p_property = process_tensor_output(out)\n",
    "    p_thing_list.append(p_thing)\n",
    "    p_property_list.append(p_property)\n",
    "print(\"inference done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thing prediction accuracy: 0.15538961698669104\n",
      "Correct thing predictions: 9177, Incorrect thing predictions: 49881\n",
      "Property prediction accuracy: 0.14441735243320125\n",
      "Correct property predictions: 8529, Incorrect property predictions: 50529\n"
     ]
    }
   ],
   "source": [
    "answer_thing = [item['answer_thing'] for item in test_dataset[\"translation\"]]\n",
    "answer_property = [item['answer_property'] for item in test_dataset[\"translation\"]]\n",
    "\n",
    "def correctness_test(input, reference):\n",
    "    assert(len(input) == len(reference))\n",
    "    correctness_list = []\n",
    "    for i in range(len(input)):\n",
    "        correctness_list.append(input[i] == reference[i])\n",
    "    return correctness_list\n",
    "\n",
    "# Compare with answer to evaluate correctness\n",
    "thing_correctness = correctness_test(p_thing_list, answer_thing)\n",
    "property_correctness = correctness_test(p_property_list, answer_property)\n",
    "\n",
    "# Calculate accuracy\n",
    "thing_accuracy = sum(thing_correctness) / len(thing_correctness)\n",
    "property_accuracy = sum(property_correctness) / len(property_correctness)\n",
    "\n",
    "# Count True/False values\n",
    "thing_true_count = thing_correctness.count(True)\n",
    "thing_false_count = thing_correctness.count(False)\n",
    "property_true_count = property_correctness.count(True)\n",
    "property_false_count = property_correctness.count(False)\n",
    "\n",
    "# Print results\n",
    "print(\"Thing prediction accuracy:\", thing_accuracy)\n",
    "print(f\"Correct thing predictions: {thing_true_count}, Incorrect thing predictions: {thing_false_count}\")\n",
    "print(\"Property prediction accuracy:\", property_accuracy)\n",
    "print(f\"Correct property predictions: {property_true_count}, Incorrect property predictions: {property_false_count}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "dict = {\n",
    "    'p_thing': p_thing_list,\n",
    "    'p_property': p_property_list,\n",
    "    'p_thing_correct': thing_correctness,\n",
    "    'p_property_correct': property_correctness\n",
    "}\n",
    "df_pred = pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with predictions saved to ../evaluation/test_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with the results\n",
    "df_pred = pd.DataFrame({\n",
    "    'p_thing': p_thing_list,\n",
    "    'p_property': p_property_list,\n",
    "    'p_thing_correct': thing_correctness,\n",
    "    'p_property_correct': property_correctness\n",
    "})\n",
    "\n",
    "# Merge predictions with the original DataFrame (df_org)\n",
    "df_org['p_thing'] = df_pred['p_thing']\n",
    "df_org['p_property'] = df_pred['p_property']\n",
    "df_org['p_thing_correct'] = df_pred['p_thing_correct']\n",
    "df_org['p_property_correct'] = df_pred['p_property_correct']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = \"../evaluation/test_with_predictions.csv\"\n",
    "df_org.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated DataFrame with predictions saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
